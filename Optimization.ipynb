{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(T, alpha, beta, sigma, alpha_y, beta_y, gamma, theta, sigma_y, mu):\n",
    "    df = pd.DataFrame(index=range(T), columns=['x', 'y', 'epsilon_x', 'epsilon_y'])\n",
    "    x1 = alpha/(1-beta) # define initial value for time series x\n",
    "    y1 = 0\n",
    "    epsilon_x = sigma*np.random.randn(T) + mu # generate a vector of T random normal\n",
    "    epsilon_y = sigma_y*np.random.randn(T) # generate a vector of T random normal\n",
    "    \n",
    "    df.iloc[0,:] = [x1, y1, np.nan, np.nan] # initialize x and y\n",
    "    \n",
    "    x = np.zeros(T)\n",
    "    y = np.zeros(T)\n",
    "    x[0] = x1\n",
    "    y[0] = y1\n",
    "    \n",
    "    for i in range(0, T-1):\n",
    "        x[i+1] = alpha + beta * x[i] + epsilon_x[i+1] # generate X(t) recursively\n",
    "            # Xt = α + βXt-1 + εt\n",
    "        y[i+1] = alpha_y + beta_y * y[i] + gamma * x[i+1] + epsilon_y[i+1] + theta * epsilon_y[i] # generate Y(t) recursively\n",
    "            # Yt = α + βYt-1 + γXt + εt + θεt-1\n",
    "        df.iloc[i+1,0] = x[i+1]\n",
    "        df.iloc[i+1,1] = y[i+1]\n",
    "        df.iloc[i+1,2] = epsilon_x[i+1] # associate error with X(t)\n",
    "        df.iloc[i+1,3] = epsilon_y[i+1] # associate error with Y(t)\n",
    "    \n",
    "    return df, x, y, epsilon_x, epsilon_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, x, y, epsilon_x, epsilon_y = generate_data(1000, 1, 0.9, 0.1, 1, 0.9, 0.9, 0.5, 0.1, 0)\n",
    "new_df = df.drop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f2/mjfy7nsn0tqbsmsk74mmlybh0000gn/T/ipykernel_24253/2053824432.py:25: RuntimeWarning: overflow encountered in exp\n",
      "  sigma_sq, rho = np.exp(params)\n",
      "/var/folders/f2/mjfy7nsn0tqbsmsk74mmlybh0000gn/T/ipykernel_24253/2053824432.py:36: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  log_likelihood += 0.5*np.log(denominator) - 0.5*(rho**2) / denominator / sigma_sq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.0862573]\n",
      "[[-685687.94293896]]\n"
     ]
    }
   ],
   "source": [
    "######################## quantile GLS normal errors\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Define the quantile of interest\n",
    "tau = 0.5\n",
    "\n",
    "# Define the weight function\n",
    "#def weight_function(residuals):\n",
    "    #return np.diag(np.exp(-0.5 * (residuals / norm.ppf(tau))**2))\n",
    "#Huber weight function, which down-weights the residuals that are larger than a certain threshold\n",
    "#if the weight matrix is singular. if the quantile of interest is too extreme (e.g., close to 0 or 1), as the weight matrix can become degenerate and hence singular.\n",
    "#solution: increase sample size or\n",
    "def huber_weight_function(residuals, c=1.345):\n",
    "    weights = np.zeros_like(residuals)\n",
    "    mask = np.abs(residuals) <= c\n",
    "    weights[mask] = 1\n",
    "    weights[~mask] = c / np.abs(residuals[~mask])\n",
    "    return np.diag(weights)\n",
    "\n",
    "\n",
    "# Define the GLS function\n",
    "def gls_quantile(x, y, tau, weight_function):\n",
    "    # Define the negative log-likelihood function\n",
    "    def neg_log_likelihood(params):\n",
    "    \n",
    "        sigma_sq, rho = np.exp(params)\n",
    "        residuals = y - np.dot(x, beta)  # Compute the residuals\n",
    "        weights = weight_function(residuals)  # Compute the weight matrix\n",
    "        det = np.linalg.det(weights)  # Compute the determinant of the weight matrix\n",
    "        inv_weights = np.linalg.inv(weights)  # Compute the inverse of the weight matrix\n",
    "        log_likelihood = 0.5*np.log(det) - 0.5*np.dot(residuals.T, np.dot(inv_weights, residuals)) / sigma_sq\n",
    "        #log_likelihood += 0.5*np.log(1 - rho**2) - 0.5*(rho**2) / (1 - rho**2) / sigma_sq\n",
    "        eps = 1e-10  # Add small positive value to denominator\n",
    "        #log_likelihood += 0.5*np.log(np.abs(1 - rho**2) + eps) - 0.5*(rho**2) / (np.abs(1 - rho**2) + eps) / sigma_sq\n",
    "\n",
    "        denominator = np.abs(1 - rho**2) + eps\n",
    "        log_likelihood += 0.5*np.log(denominator) - 0.5*(rho**2) / denominator / sigma_sq\n",
    "\n",
    "        log_likelihood += 0.5*np.log(sigma_sq)  # Add the log of the variance term\n",
    "        return -log_likelihood  # Return the negative log-likelihood\n",
    "\n",
    "    # Initialize the coefficients with OLS\n",
    "    beta = np.linalg.lstsq(x, y, rcond=None)[0]\n",
    "\n",
    "    # Optimize the negative log-likelihood function\n",
    "    res = minimize(neg_log_likelihood, np.array([1, 0]), method='L-BFGS-B')\n",
    "    sigma_sq, rho = np.exp(res.x)\n",
    "\n",
    "    # Compute the residuals and the weight matrix\n",
    "    residuals = y - np.dot(x, beta)\n",
    "    weights = weight_function(residuals)\n",
    "\n",
    "    # Compute the GLS estimator\n",
    "    det = np.linalg.det(weights)\n",
    "    inv_weights = np.linalg.inv(weights)\n",
    "    beta_gls = np.linalg.lstsq(np.dot(x.T, inv_weights).dot(x), np.dot(x.T, inv_weights).dot(y), rcond=None)[0]\n",
    "\n",
    "    # Compute the GLS variance\n",
    "    weighted_resid = np.dot(residuals.T, inv_weights)\n",
    "    var_gls = np.dot(weighted_resid, weighted_resid.T) / (len(y) - x.shape[1])\n",
    "    var_gls *= (1 - rho**2) / sigma_sq\n",
    "    var_gls += np.diag(np.ones(x.shape[1]))*rho**2 / sigma_sq\n",
    "\n",
    "    print(beta_gls)\n",
    "    print(var_gls)\n",
    "    return beta_gls, var_gls\n",
    "\n",
    "#x = np.random.normal(size=100)\n",
    "x = x.reshape(-1, 1)  # Reshape to a 2D array\n",
    "#y = 1 + 2*x[:, 0] + np.random.normal(size=100)\n",
    "\n",
    "# Fit a quantile regression model with GLS assuming normal errors\n",
    "#beta, var = gls_quantile(np.column_stack((np.ones_like(x), x)), y, tau, weight_function)\n",
    "\n",
    "beta, var = gls_quantile(x, y, tau, huber_weight_function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f2/mjfy7nsn0tqbsmsk74mmlybh0000gn/T/ipykernel_24253/3048928935.py:23: RuntimeWarning: invalid value encountered in log\n",
      "  log_likelihood = np.sum(np.log(np.diag(weights))) - np.sum(np.abs(residuals) / sigma_sq)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.00430583]\n",
      "[[0.36787944]]\n"
     ]
    }
   ],
   "source": [
    "############ Quantile GLS Laplace error Weight\n",
    "# Define the quantile of interest\n",
    "tau = 0.5\n",
    "\n",
    "\n",
    "def laplace_weight_function(residuals, b=1):\n",
    "    #weights = np.exp(-np.abs(residuals) / b)\n",
    "    weights = np.random.laplace(-np.abs(residuals) / b)\n",
    "    return np.diag(weights)\n",
    "\n",
    "# Define the GLS function\n",
    "def gls_quantile(x, y, tau, weight_function):\n",
    "    # Define the negative log-likelihood function\n",
    "    def neg_log_likelihood(params):\n",
    "    \n",
    "        sigma_sq, rho = np.exp(params)\n",
    "        residuals = y - np.dot(x, beta)  # Compute the residuals\n",
    "        weights = weight_function(residuals)  # Compute the weight matrix\n",
    "        det = np.linalg.det(weights)  # Compute the determinant of the weight matrix\n",
    "        inv_weights = np.linalg.inv(weights)  # Compute the inverse of the weight matrix\n",
    "        \n",
    "        #log_likelihood = 0.5*np.log(det) - 0.5*np.dot(residuals.T, np.dot(inv_weights, residuals)) / sigma_sq\n",
    "        log_likelihood = np.sum(np.log(np.diag(weights))) - np.sum(np.abs(residuals) / sigma_sq)\n",
    "        #log_likelihood += 0.5*np.log(1 - rho**2) - 0.5*(rho**2) / (1 - rho**2) / sigma_sq\n",
    "        eps = 1e-10  # Add small positive value to denominator\n",
    "        #log_likelihood += 0.5*np.log(np.abs(1 - rho**2) + eps) - 0.5*(rho**2) / (np.abs(1 - rho**2) + eps) / sigma_sq\n",
    "\n",
    "        denominator = np.abs(1 - rho**2) + eps\n",
    "        log_likelihood += 0.5*np.log(denominator) - 0.5*(rho**2) / denominator / sigma_sq\n",
    "\n",
    "        log_likelihood += 0.5*np.log(sigma_sq)  # Add the log of the variance term\n",
    "        return -log_likelihood  # Return the negative log-likelihood\n",
    "\n",
    "    # Initialize the coefficients with OLS\n",
    "    beta = np.linalg.lstsq(x, y, rcond=None)[0]\n",
    "\n",
    "    # Optimize the negative log-likelihood function\n",
    "    res = minimize(neg_log_likelihood, np.array([1, 0]), method='L-BFGS-B')\n",
    "    sigma_sq, rho = np.exp(res.x)\n",
    "\n",
    "    # Compute the residuals and the weight matrix\n",
    "    residuals = y - np.dot(x, beta)\n",
    "    weights = weight_function(residuals)\n",
    "\n",
    "    # Compute the GLS estimator\n",
    "    det = np.linalg.det(weights)\n",
    "    inv_weights = np.linalg.inv(weights)\n",
    "    beta_gls = np.linalg.lstsq(np.dot(x.T, inv_weights).dot(x), np.dot(x.T, inv_weights).dot(y), rcond=None)[0]\n",
    "\n",
    "    # Compute the GLS variance\n",
    "    weighted_resid = np.dot(residuals.T, inv_weights)\n",
    "    var_gls = np.dot(weighted_resid, weighted_resid.T) / (len(y) - x.shape[1])\n",
    "    var_gls *= (1 - rho**2) / sigma_sq\n",
    "    var_gls += np.diag(np.ones(x.shape[1]))*rho**2 / sigma_sq\n",
    "\n",
    "    print(beta_gls)\n",
    "    print(var_gls)\n",
    "    return beta_gls, var_gls\n",
    "\n",
    "#x = np.random.normal(size=100)\n",
    "x = x.reshape(-1, 1)  # Reshape to a 2D array\n",
    "#y = 1 + 2*x[:, 0] + np.random.normal(size=100)\n",
    "\n",
    "# Fit a quantile regression model with GLS assuming normal errors\n",
    "#beta, var = gls_quantile(np.column_stack((np.ones_like(x), x)), y, tau, weight_function)\n",
    "\n",
    "beta, var = gls_quantile(x, y, tau, laplace_weight_function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.53041155  0.32132771  1.23810895  1.54101256  0.14971553  0.48723179\n",
      "  0.46741677  0.60508731  0.05261598 -0.55356941 -1.28252434 -0.30621721\n",
      "  0.46985889  0.68882266 -0.83220722  0.16499098  0.50864633  1.92775326\n",
      "  0.41388462 -0.01737432  0.20905304 -0.22761676 -0.24441633  1.02625057\n",
      " -2.23452663 -0.29726811  1.06953428 -0.61516498  0.43304716 -0.00645704\n",
      " -0.37167    -0.10189762 -0.56883341  0.61580813 -1.20836582  0.16341879\n",
      " -0.52498936  0.27987305  1.10901069 -0.02712289 -0.37619653  1.10208722\n",
      "  0.63020956  1.51020602  1.22627796  1.38883976 -0.5539065  -0.25087808\n",
      " -0.10508656  0.49437655  0.02701274  1.55491191 -0.82448652 -0.607733\n",
      "  0.09896388 -1.1933123   0.38074981 -0.18780691 -1.05352566 -0.85471157\n",
      "  0.19270253 -0.31400145  1.09804135 -0.20997025 -0.10221741 -1.07137673\n",
      "  0.60208051 -0.28117304  0.70475358 -0.50626457  1.43569403  0.08478007\n",
      "  0.68041092  1.27466326 -0.3270033  -0.0318273   0.80086974 -0.0519348\n",
      "  2.5098065  -0.21809258 -0.87709725 -1.03808695  0.75181666 -1.24878429\n",
      "  0.01377873  1.06340561  0.52580312 -0.28983625  0.60477266  1.30133374\n",
      "  0.68690729 -0.05758591 -0.11179939 -3.16362759  2.30306442 -0.9609135\n",
      "  1.94029862  0.14232129  1.57768657 -1.09404635]\n"
     ]
    }
   ],
   "source": [
    "l = 1\n",
    "T = 100\n",
    "z = np.random.exponential(l, T)\n",
    "V = np.random.normal(0, 1, T)\n",
    "\n",
    "#G = #simmetric covariance matrix\n",
    "mu = 0.2\n",
    "errors = mu + np.sqrt(z) * V #* np.sqrt(G)\n",
    "print(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Quantile GLS Laplace error WITH NON-ZERO CORRELATION\n",
    "# Define the quantile of interest\n",
    "tau = 0.5\n",
    "\n",
    "def laplace_weight_function(residuals, b=1):\n",
    "    #weights = np.exp(-np.abs(residuals) / b)\n",
    "    weights = np.random.laplace(-np.abs(residuals) / b)\n",
    "    return np.diag(weights)\n",
    "\n",
    "\n",
    "# Define the GLS function\n",
    "def gls_quantile(x, y, tau, weight_function):\n",
    "    # Define the negative log-likelihood function\n",
    "    def neg_log_likelihood(params):\n",
    "    \n",
    "        sigma_sq, rho = np.exp(params)\n",
    "        residuals = y - np.dot(x, beta)  # Compute the residuals\n",
    "        weights = weight_function(residuals)  # Compute the weight matrix\n",
    "        det = np.linalg.det(weights)  # Compute the determinant of the weight matrix\n",
    "        inv_weights = np.linalg.inv(weights)  # Compute the inverse of the weight matrix\n",
    "        \n",
    "        #log_likelihood = 0.5*np.log(det) - 0.5*np.dot(residuals.T, np.dot(inv_weights, residuals)) / sigma_sq\n",
    "        log_likelihood = np.sum(np.log(np.diag(weights))) - np.sum(np.abs(residuals) / sigma_sq)\n",
    "        #log_likelihood += 0.5*np.log(1 - rho**2) - 0.5*(rho**2) / (1 - rho**2) / sigma_sq\n",
    "        eps = 1e-10  # Add small positive value to denominator\n",
    "        #log_likelihood += 0.5*np.log(np.abs(1 - rho**2) + eps) - 0.5*(rho**2) / (np.abs(1 - rho**2) + eps) / sigma_sq\n",
    "\n",
    "        denominator = np.abs(1 - rho**2) + eps\n",
    "        log_likelihood += 0.5*np.log(denominator) - 0.5*(rho**2) / denominator / sigma_sq\n",
    "\n",
    "        log_likelihood += 0.5*np.log(sigma_sq)  # Add the log of the variance term\n",
    "        return -log_likelihood  # Return the negative log-likelihood\n",
    "\n",
    "    # Initialize the coefficients with OLS\n",
    "    beta = np.linalg.lstsq(x, y, rcond=None)[0]\n",
    "\n",
    "    # Optimize the negative log-likelihood function\n",
    "    res = minimize(neg_log_likelihood, np.array([1, 0]), method='L-BFGS-B')\n",
    "    sigma_sq, rho = np.exp(res.x)\n",
    "\n",
    "    # Compute the residuals and the weight matrix\n",
    "    residuals = y - np.dot(x, beta)\n",
    "    weights = weight_function(residuals)\n",
    "\n",
    "    # Compute the GLS estimator\n",
    "    det = np.linalg.det(weights)\n",
    "    inv_weights = np.linalg.inv(weights)\n",
    "    beta_gls = np.linalg.lstsq(np.dot(x.T, inv_weights).dot(x), np.dot(x.T, inv_weights).dot(y), rcond=None)[0]\n",
    "\n",
    "    # Compute the GLS variance\n",
    "    weighted_resid = np.dot(residuals.T, inv_weights)\n",
    "    var_gls = np.dot(weighted_resid, weighted_resid.T) / (len(y) - x.shape[1])\n",
    "    var_gls *= (1 - rho**2) / sigma_sq\n",
    "    var_gls += np.diag(np.ones(x.shape[1]))*rho**2 / sigma_sq\n",
    "\n",
    "    print(beta_gls)\n",
    "    print(var_gls)\n",
    "    return beta_gls, var_gls\n",
    "\n",
    "#x = np.random.normal(size=100)\n",
    "x = x.reshape(-1, 1)  # Reshape to a 2D array\n",
    "#y = 1 + 2*x[:, 0] + np.random.normal(size=100)\n",
    "\n",
    "# Fit a quantile regression model with GLS assuming normal errors\n",
    "#beta, var = gls_quantile(np.column_stack((np.ones_like(x), x)), y, tau, weight_function)\n",
    "\n",
    "beta, var = gls_quantile(x, y, tau, laplace_weight_function)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f2/mjfy7nsn0tqbsmsk74mmlybh0000gn/T/ipykernel_24253/1295906856.py:15: RuntimeWarning: divide by zero encountered in divide\n",
      "  return np.diag(np.exp(-0.5 * (residuals / norm.ppf(tau))**2))\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/tatiana/Documents/GitHub/GLS-Quantile-Regression/Optimization.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 75>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tatiana/Documents/GitHub/GLS-Quantile-Regression/Optimization.ipynb#X25sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39m#y = 1 + 2*x[:, 0] + np.random.normal(size=100)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tatiana/Documents/GitHub/GLS-Quantile-Regression/Optimization.ipynb#X25sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tatiana/Documents/GitHub/GLS-Quantile-Regression/Optimization.ipynb#X25sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m \u001b[39m# Fit a quantile regression model with GLS assuming normal errors\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tatiana/Documents/GitHub/GLS-Quantile-Regression/Optimization.ipynb#X25sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m \u001b[39m#beta, var = gls_quantile(np.column_stack((np.ones_like(x), x)), y, tau, weight_function)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tatiana/Documents/GitHub/GLS-Quantile-Regression/Optimization.ipynb#X25sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m y \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39m2\u001b[39m\u001b[39m*\u001b[39mx[:, \u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m laplace_errors(\u001b[39m1000\u001b[39m, scale\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tatiana/Documents/GitHub/GLS-Quantile-Regression/Optimization.ipynb#X25sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m beta, var \u001b[39m=\u001b[39m gls_quantile(np\u001b[39m.\u001b[39;49mcolumn_stack((np\u001b[39m.\u001b[39;49mones_like(x), x)), y, tau, weight_function)\n",
      "\u001b[1;32m/Users/tatiana/Documents/GitHub/GLS-Quantile-Regression/Optimization.ipynb Cell 6\u001b[0m in \u001b[0;36mgls_quantile\u001b[0;34m(x, y, tau, weight_function)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tatiana/Documents/GitHub/GLS-Quantile-Regression/Optimization.ipynb#X25sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m beta \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mlstsq(x, y, rcond\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tatiana/Documents/GitHub/GLS-Quantile-Regression/Optimization.ipynb#X25sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39m# Optimize the negative log-likelihood function\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tatiana/Documents/GitHub/GLS-Quantile-Regression/Optimization.ipynb#X25sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m res \u001b[39m=\u001b[39m minimize(neg_log_likelihood, np\u001b[39m.\u001b[39;49marray([\u001b[39m1\u001b[39;49m, \u001b[39m0\u001b[39;49m]), method\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mL-BFGS-B\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tatiana/Documents/GitHub/GLS-Quantile-Regression/Optimization.ipynb#X25sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m sigma_sq, rho \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexp(res\u001b[39m.\u001b[39mx)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tatiana/Documents/GitHub/GLS-Quantile-Regression/Optimization.ipynb#X25sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39m# Compute the residuals and the weight matrix\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scipy/optimize/_minimize.py:696\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    693\u001b[0m     res \u001b[39m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[1;32m    694\u001b[0m                              \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[1;32m    695\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39ml-bfgs-b\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 696\u001b[0m     res \u001b[39m=\u001b[39m _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[1;32m    697\u001b[0m                            callback\u001b[39m=\u001b[39;49mcallback, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptions)\n\u001b[1;32m    698\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtnc\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    699\u001b[0m     res \u001b[39m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[39m=\u001b[39mcallback,\n\u001b[1;32m    700\u001b[0m                         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scipy/optimize/_lbfgsb_py.py:305\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    303\u001b[0m         iprint \u001b[39m=\u001b[39m disp\n\u001b[0;32m--> 305\u001b[0m sf \u001b[39m=\u001b[39m _prepare_scalar_function(fun, x0, jac\u001b[39m=\u001b[39;49mjac, args\u001b[39m=\u001b[39;49margs, epsilon\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    306\u001b[0m                               bounds\u001b[39m=\u001b[39;49mnew_bounds,\n\u001b[1;32m    307\u001b[0m                               finite_diff_rel_step\u001b[39m=\u001b[39;49mfinite_diff_rel_step)\n\u001b[1;32m    309\u001b[0m func_and_grad \u001b[39m=\u001b[39m sf\u001b[39m.\u001b[39mfun_and_grad\n\u001b[1;32m    311\u001b[0m fortran_int \u001b[39m=\u001b[39m _lbfgsb\u001b[39m.\u001b[39mtypes\u001b[39m.\u001b[39mintvar\u001b[39m.\u001b[39mdtype\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scipy/optimize/_optimize.py:332\u001b[0m, in \u001b[0;36m_prepare_scalar_function\u001b[0;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[1;32m    328\u001b[0m     bounds \u001b[39m=\u001b[39m (\u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39minf, np\u001b[39m.\u001b[39minf)\n\u001b[1;32m    330\u001b[0m \u001b[39m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[39m# calculation reduces overall function evaluations.\u001b[39;00m\n\u001b[0;32m--> 332\u001b[0m sf \u001b[39m=\u001b[39m ScalarFunction(fun, x0, args, grad, hess,\n\u001b[1;32m    333\u001b[0m                     finite_diff_rel_step, bounds, epsilon\u001b[39m=\u001b[39;49mepsilon)\n\u001b[1;32m    335\u001b[0m \u001b[39mreturn\u001b[39;00m sf\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:158\u001b[0m, in \u001b[0;36mScalarFunction.__init__\u001b[0;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf \u001b[39m=\u001b[39m fun_wrapped(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx)\n\u001b[1;32m    157\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_fun_impl \u001b[39m=\u001b[39m update_fun\n\u001b[0;32m--> 158\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_fun()\n\u001b[1;32m    160\u001b[0m \u001b[39m# Gradient evaluation\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[39mif\u001b[39;00m callable(grad):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update_fun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    250\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_updated:\n\u001b[0;32m--> 251\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_fun_impl()\n\u001b[1;32m    252\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_updated \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate_fun\u001b[39m():\n\u001b[0;32m--> 155\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf \u001b[39m=\u001b[39m fun_wrapped(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnfev \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    134\u001b[0m \u001b[39m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[39m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[39m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m fx \u001b[39m=\u001b[39m fun(np\u001b[39m.\u001b[39;49mcopy(x), \u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    138\u001b[0m \u001b[39m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39misscalar(fx):\n",
      "\u001b[1;32m/Users/tatiana/Documents/GitHub/GLS-Quantile-Regression/Optimization.ipynb Cell 6\u001b[0m in \u001b[0;36mgls_quantile.<locals>.neg_log_likelihood\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tatiana/Documents/GitHub/GLS-Quantile-Regression/Optimization.ipynb#X25sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m weights \u001b[39m=\u001b[39m weight_function(residuals)  \u001b[39m# Compute the weight matrix\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tatiana/Documents/GitHub/GLS-Quantile-Regression/Optimization.ipynb#X25sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m det \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mdet(weights)  \u001b[39m# Compute the determinant of the weight matrix\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tatiana/Documents/GitHub/GLS-Quantile-Regression/Optimization.ipynb#X25sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m inv_weights \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mlinalg\u001b[39m.\u001b[39;49minv(weights)  \u001b[39m# Compute the inverse of the weight matrix\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tatiana/Documents/GitHub/GLS-Quantile-Regression/Optimization.ipynb#X25sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m log_likelihood \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(np\u001b[39m.\u001b[39mlog(\u001b[39m0.5\u001b[39m\u001b[39m/\u001b[39msigma_sq) \u001b[39m-\u001b[39m np\u001b[39m.\u001b[39mabs(residuals)\u001b[39m/\u001b[39msigma_sq) \u001b[39m-\u001b[39m np\u001b[39m.\u001b[39msum(np\u001b[39m.\u001b[39mlog(np\u001b[39m.\u001b[39mdiag(weights)))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tatiana/Documents/GitHub/GLS-Quantile-Regression/Optimization.ipynb#X25sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m log_likelihood \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m\u001b[39m*\u001b[39mnp\u001b[39m.\u001b[39mlog(np\u001b[39m.\u001b[39mabs(\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m rho\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m) \u001b[39m+\u001b[39m eps) \u001b[39m-\u001b[39m \u001b[39m0.5\u001b[39m\u001b[39m*\u001b[39m(rho\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m) \u001b[39m/\u001b[39m (np\u001b[39m.\u001b[39mabs(\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m rho\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m) \u001b[39m+\u001b[39m eps) \u001b[39m/\u001b[39m sigma_sq\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36minv\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/numpy/linalg/linalg.py:552\u001b[0m, in \u001b[0;36minv\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m    550\u001b[0m signature \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mD->D\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m isComplexType(t) \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39md->d\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    551\u001b[0m extobj \u001b[39m=\u001b[39m get_linalg_error_extobj(_raise_linalgerror_singular)\n\u001b[0;32m--> 552\u001b[0m ainv \u001b[39m=\u001b[39m _umath_linalg\u001b[39m.\u001b[39;49minv(a, signature\u001b[39m=\u001b[39;49msignature, extobj\u001b[39m=\u001b[39;49mextobj)\n\u001b[1;32m    553\u001b[0m \u001b[39mreturn\u001b[39;00m wrap(ainv\u001b[39m.\u001b[39mastype(result_t, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/numpy/linalg/linalg.py:89\u001b[0m, in \u001b[0;36m_raise_linalgerror_singular\u001b[0;34m(err, flag)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_raise_linalgerror_singular\u001b[39m(err, flag):\n\u001b[0;32m---> 89\u001b[0m     \u001b[39mraise\u001b[39;00m LinAlgError(\u001b[39m\"\u001b[39m\u001b[39mSingular matrix\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "######################## ############ Quantile GLS Laplace error WRONG?\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Define the quantile of interest\n",
    "tau = 0.5\n",
    "\n",
    "# Define the weight function\n",
    "#def weight_function(residuals):\n",
    "    #return np.diag(np.exp(-0.5 * (residuals / norm.ppf(tau))**2))\n",
    "def laplace_errors(n, loc=0, scale=1):\n",
    "    return np.random.laplace(loc=loc, scale=scale, size=n)\n",
    "\n",
    "\n",
    "def huber_weight_function(residuals, c=1.345):\n",
    "    weights = np.zeros_like(residuals)\n",
    "    mask = np.abs(residuals) <= c\n",
    "    weights[mask] = 1\n",
    "    weights[~mask] = c / np.abs(residuals[~mask])\n",
    "    return np.diag(weights)\n",
    "\n",
    "\n",
    "# Define the GLS function\n",
    "def gls_quantile(x, y, tau, weight_function):\n",
    "    # Define the negative log-likelihood function\n",
    "    \n",
    "    def neg_log_likelihood(params):\n",
    "        sigma_sq, rho = np.exp(params)\n",
    "        residuals = y - np.dot(x, beta)  # Compute the residuals\n",
    "        weights = weight_function(residuals)  # Compute the weight matrix\n",
    "        det = np.linalg.det(weights)  # Compute the determinant of the weight matrix\n",
    "        inv_weights = np.linalg.inv(weights)  # Compute the inverse of the weight matrix\n",
    "        log_likelihood = np.sum(np.log(0.5/sigma_sq) - np.abs(residuals)/sigma_sq) - np.sum(np.log(np.diag(weights)))\n",
    "        log_likelihood += 0.5*np.log(np.abs(1 - rho**2) + eps) - 0.5*(rho**2) / (np.abs(1 - rho**2) + eps) / sigma_sq\n",
    "        log_likelihood += 0.5*np.log(sigma_sq)  # Add the log of the variance term\n",
    "        return -log_likelihood  # Return the negative log-likelihood\n",
    "\n",
    "\n",
    "    # Initialize the coefficients with OLS\n",
    "    beta = np.linalg.lstsq(x, y, rcond=None)[0]\n",
    "\n",
    "    # Optimize the negative log-likelihood function\n",
    "    res = minimize(neg_log_likelihood, np.array([1, 0]), method='L-BFGS-B')\n",
    "    sigma_sq, rho = np.exp(res.x)\n",
    "\n",
    "    # Compute the residuals and the weight matrix\n",
    "    residuals = y - np.dot(x, beta)\n",
    "    weights = weight_function(residuals)\n",
    "\n",
    "    # Compute the GLS estimator\n",
    "    det = np.linalg.det(weights)\n",
    "    inv_weights = np.linalg.inv(weights)\n",
    "    beta_gls = np.linalg.lstsq(np.dot(x.T, inv_weights).dot(x), np.dot(x.T, inv_weights).dot(y), rcond=None)[0]\n",
    "\n",
    "    # Compute the GLS variance\n",
    "    weighted_resid = np.dot(residuals.T, inv_weights)\n",
    "    var_gls = np.dot(weighted_resid, weighted_resid.T) / (len(y) - x.shape[1])\n",
    "    var_gls *= (1 - rho**2) / sigma_sq\n",
    "    var_gls += np.diag(np.ones(x.shape[1]))*rho**2 / sigma_sq\n",
    "\n",
    "    print(beta_gls)\n",
    "    print(var_gls)\n",
    "\n",
    "    return beta_gls, var_gls\n",
    "\n",
    "#x = np.random.normal(size=100)\n",
    "x = x.reshape(-1, 1)  # Reshape to a 2D array\n",
    "#y = 1 + 2*x[:, 0] + np.random.normal(size=100)\n",
    "\n",
    "# Fit a quantile regression model with GLS assuming normal errors\n",
    "#beta, var = gls_quantile(np.column_stack((np.ones_like(x), x)), y, tau, weight_function)\n",
    "\n",
    "y = 1 + 2*x[:, 0] + laplace_errors(1000, scale=1)\n",
    "beta, var = gls_quantile(np.column_stack((np.ones_like(x), x)), y, tau, weight_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def neg_log_likelihood(params, X, y, tau):\n",
    "    n = X.shape[0]\n",
    "    sigma_sq, rho = np.exp(params[:2])\n",
    "    omega = np.identity(n) * (1 - rho**2)\n",
    "    beta = params[2:]\n",
    "    residuals = y - np.dot(X, beta)\n",
    "    Q = np.dot(X.T, np.dot(omega, X))\n",
    "    Q_inv = np.linalg.inv(Q)\n",
    "    beta_hat = np.dot(Q_inv, np.dot(X.T, np.dot(omega, y)))\n",
    "    e = y - np.dot(X, beta_hat)\n",
    "    u = e / sigma_sq\n",
    "    ll = tau * np.sum(np.maximum(u, 0)) + (1 - tau) * np.sum(np.maximum(-u, 0))\n",
    "    log_likelihood = -ll + np.log(sigma_sq) + np.log(np.abs(1 - rho**2) + 1e-10)\n",
    "    return log_likelihood\n",
    "\n",
    "def fit_quantile_regression_gls(X, y, tau, initial_params=None):\n",
    "    n, k = X.shape\n",
    "    if initial_params is None:\n",
    "        initial_params = np.zeros(k + 2)\n",
    "    res = minimize(neg_log_likelihood, initial_params, args=(X, y, tau))\n",
    "    params = res.x\n",
    "    sigma_sq, rho = np.exp(params[:2])\n",
    "    beta = params[2:]\n",
    "    return beta, sigma_sq, rho\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "def quantile_regression_gls(y, X, tau):\n",
    "    n, p = X.shape\n",
    "    nq = len(tau)\n",
    "    residuals = np.zeros((n, nq))\n",
    "    beta = np.zeros((nq, p))\n",
    "    weights = np.zeros((n, n))\n",
    "    Xw = np.zeros((n, nq*p+1))\n",
    "\n",
    "    for i in range(nq):\n",
    "        q = tau[i]\n",
    "        for j in range(n):\n",
    "            residuals[j, i] = y[j] - np.dot(X[j,:], beta[i,:])\n",
    "            weights[j,j] = norm.pdf(residuals[j,i]/weights[j,j])\n",
    "        Xw[:, i*p+1:(i+1)*p+1] = np.multiply(X, np.tile(weights[:,i],(p,1)).T)\n",
    "        #Xw[:, i*p] = np.tile(weights[:,i],(1,1)).T\n",
    "        Xw[:, i*p+1:(i+1)*p+1] = np.multiply(X, np.tile(weights[:,i, np.newaxis],(1,p)))\n",
    "\n",
    "        #debugging\n",
    "        rank = np.linalg.matrix_rank(Xw.T @ Xw)\n",
    "        #beta[i,:] = np.linalg.inv(Xw.T @ Xw) @ Xw.T @ np.multiply(y, np.tile(weights[:,i],(1,1)).T)\n",
    "\n",
    "    #return beta\n",
    "    return rank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 4)\n",
      "(3, 3)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(beta.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f2/mjfy7nsn0tqbsmsk74mmlybh0000gn/T/ipykernel_24253/3250660834.py:16: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  weights[j,j] = norm.pdf(residuals[j,i]/weights[j,j])\n",
      "/var/folders/f2/mjfy7nsn0tqbsmsk74mmlybh0000gn/T/ipykernel_24253/3250660834.py:16: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  weights[j,j] = norm.pdf(residuals[j,i]/weights[j,j])\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "SVD did not converge",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/tatiana/Documents/GitHub/GLS-Quantile-Regression/Optimization.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tatiana/Documents/GitHub/GLS-Quantile-Regression/Optimization.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m X \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mcolumn_stack((np\u001b[39m.\u001b[39mones_like(x), y_lag, x, epsilon_lag)) \u001b[39m#question\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tatiana/Documents/GitHub/GLS-Quantile-Regression/Optimization.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m tau \u001b[39m=\u001b[39m [\u001b[39m0.1\u001b[39m, \u001b[39m0.5\u001b[39m, \u001b[39m0.9\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tatiana/Documents/GitHub/GLS-Quantile-Regression/Optimization.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m quantile_regression_gls(y, X, tau)\n",
      "\u001b[1;32m/Users/tatiana/Documents/GitHub/GLS-Quantile-Regression/Optimization.ipynb Cell 6\u001b[0m in \u001b[0;36mquantile_regression_gls\u001b[0;34m(y, X, tau)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tatiana/Documents/GitHub/GLS-Quantile-Regression/Optimization.ipynb#W5sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     Xw[:, i\u001b[39m*\u001b[39mp\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m:(i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m*\u001b[39mp\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmultiply(X, np\u001b[39m.\u001b[39mtile(weights[:,i, np\u001b[39m.\u001b[39mnewaxis],(\u001b[39m1\u001b[39m,p)))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tatiana/Documents/GitHub/GLS-Quantile-Regression/Optimization.ipynb#W5sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39m#debugging\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tatiana/Documents/GitHub/GLS-Quantile-Regression/Optimization.ipynb#W5sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     rank \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mlinalg\u001b[39m.\u001b[39;49mmatrix_rank(Xw\u001b[39m.\u001b[39;49mT \u001b[39m@\u001b[39;49m Xw)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tatiana/Documents/GitHub/GLS-Quantile-Regression/Optimization.ipynb#W5sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39m#beta[i,:] = np.linalg.inv(Xw.T @ Xw) @ Xw.T @ np.multiply(y, np.tile(weights[:,i],(1,1)).T)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tatiana/Documents/GitHub/GLS-Quantile-Regression/Optimization.ipynb#W5sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tatiana/Documents/GitHub/GLS-Quantile-Regression/Optimization.ipynb#W5sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m#return beta\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tatiana/Documents/GitHub/GLS-Quantile-Regression/Optimization.ipynb#W5sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mreturn\u001b[39;00m rank\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mmatrix_rank\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/numpy/linalg/linalg.py:1898\u001b[0m, in \u001b[0;36mmatrix_rank\u001b[0;34m(A, tol, hermitian)\u001b[0m\n\u001b[1;32m   1896\u001b[0m \u001b[39mif\u001b[39;00m A\u001b[39m.\u001b[39mndim \u001b[39m<\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m   1897\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mint\u001b[39m(\u001b[39mnot\u001b[39;00m \u001b[39mall\u001b[39m(A\u001b[39m==\u001b[39m\u001b[39m0\u001b[39m))\n\u001b[0;32m-> 1898\u001b[0m S \u001b[39m=\u001b[39m svd(A, compute_uv\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, hermitian\u001b[39m=\u001b[39;49mhermitian)\n\u001b[1;32m   1899\u001b[0m \u001b[39mif\u001b[39;00m tol \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1900\u001b[0m     tol \u001b[39m=\u001b[39m S\u001b[39m.\u001b[39mmax(axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, keepdims\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39m*\u001b[39m \u001b[39mmax\u001b[39m(A\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m:]) \u001b[39m*\u001b[39m finfo(S\u001b[39m.\u001b[39mdtype)\u001b[39m.\u001b[39meps\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36msvd\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/numpy/linalg/linalg.py:1669\u001b[0m, in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv, hermitian)\u001b[0m\n\u001b[1;32m   1666\u001b[0m     gufunc \u001b[39m=\u001b[39m _umath_linalg\u001b[39m.\u001b[39msvd_n\n\u001b[1;32m   1668\u001b[0m signature \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mD->d\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m isComplexType(t) \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39md->d\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m-> 1669\u001b[0m s \u001b[39m=\u001b[39m gufunc(a, signature\u001b[39m=\u001b[39;49msignature, extobj\u001b[39m=\u001b[39;49mextobj)\n\u001b[1;32m   1670\u001b[0m s \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mastype(_realType(result_t), copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m   1671\u001b[0m \u001b[39mreturn\u001b[39;00m s\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/numpy/linalg/linalg.py:98\u001b[0m, in \u001b[0;36m_raise_linalgerror_svd_nonconvergence\u001b[0;34m(err, flag)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_raise_linalgerror_svd_nonconvergence\u001b[39m(err, flag):\n\u001b[0;32m---> 98\u001b[0m     \u001b[39mraise\u001b[39;00m LinAlgError(\u001b[39m\"\u001b[39m\u001b[39mSVD did not converge\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mLinAlgError\u001b[0m: SVD did not converge"
     ]
    }
   ],
   "source": [
    "y_lag = np.roll(y, 1)\n",
    "y_lag[0] = 0\n",
    "epsilon_lag = np.roll(epsilon_y, 1)\n",
    "epsilon_lag[0] = 0\n",
    "X = np.column_stack((np.ones_like(x), y_lag, x, epsilon_lag)) #question\n",
    "tau = [0.1, 0.5, 0.9]\n",
    "quantile_regression_gls(y, X, tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Xw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/tatiana/Documents/GitHub/GLS-Quantile-Regression/Optimization.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tatiana/Documents/GitHub/GLS-Quantile-Regression/Optimization.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m rank \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mmatrix_rank(Xw\u001b[39m.\u001b[39mT \u001b[39m@\u001b[39m Xw)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Xw' is not defined"
     ]
    }
   ],
   "source": [
    "rank = np.linalg.matrix_rank(Xw.T @ Xw)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
